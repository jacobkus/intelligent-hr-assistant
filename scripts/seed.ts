#!/usr/bin/env bun
/**
 * Database Seed Script
 * Loads HR documents from content/hr/, chunks them semantically,
 * generates embeddings, and stores everything in PostgreSQL with pgvector
 */

import "dotenv/config";
import { createHash } from "node:crypto";
import { readdir, readFile } from "node:fs/promises";
import { join } from "node:path";
import matter from "gray-matter";
import { chunkMarkdownBySections } from "@/lib/ai/chunking";
import { generateEmbeddings } from "@/lib/ai/embedding";
import { db } from "@/lib/db";
import { insertChunks } from "@/lib/db/operations";
import { documents } from "@/lib/db/schema";
import { env } from "@/lib/env.mjs";

const CONTENT_DIR = join(process.cwd(), "content", "hr");

/**
 * Generate SHA-256 checksum for content deduplication
 */
function generateChecksum(content: string): string {
  return createHash("sha256").update(content).digest("hex");
}

/**
 * Load and parse a single markdown file
 */
async function loadDocument(filePath: string) {
  const fileContent = await readFile(filePath, "utf-8");
  const { data: frontmatter, content } = matter(fileContent);

  return {
    frontmatter,
    content,
    sourceFile: filePath.split("/").pop() || filePath,
    checksum: generateChecksum(fileContent),
  };
}

/**
 * Main seeding function
 */
async function seed() {
  console.log("üå± Starting database seed...\n");
  console.log("üìÅ Content directory:", CONTENT_DIR);
  console.log("üóÑÔ∏è  Database:", env.DATABASE_URL.split("@")[1]?.split("/")[0]);
  console.log("");

  try {
    // Read all markdown files from content/hr
    const files = await readdir(CONTENT_DIR);
    const mdFiles = files.filter(
      (f) => f.endsWith(".md") && !f.startsWith("_"),
    );

    console.log(`üìÑ Found ${mdFiles.length} markdown files\n`);

    let totalDocuments = 0;
    let totalChunks = 0;
    let totalEmbeddings = 0;
    let skippedDuplicates = 0;

    for (const file of mdFiles) {
      const filePath = join(CONTENT_DIR, file);
      console.log(`üìñ Processing: ${file}`);

      // Load and parse document
      const doc = await loadDocument(filePath);
      const title = doc.frontmatter.title || file.replace(".md", "");

      // Check if document already exists (idempotent seeding)
      // Note: We use db.insert directly here because insertDocument() doesn't support onConflictDoNothing
      // Validation is still enforced by the schema
      const [existingDoc] = await db
        .insert(documents)
        .values({
          checksum: doc.checksum,
          sourceFile: doc.sourceFile,
          title,
        })
        .onConflictDoNothing({ target: documents.checksum })
        .returning({ id: documents.id });

      if (!existingDoc) {
        console.log(`  ‚è≠Ô∏è  Skipped (already exists)`);
        skippedDuplicates++;
        continue;
      }

      totalDocuments++;

      // Chunk the content
      const documentChunks = chunkMarkdownBySections(doc.content);
      console.log(`  ‚úÇÔ∏è  Created ${documentChunks.length} chunks`);

      // Generate embeddings for all chunks
      const chunkContents = documentChunks.map((c) => c.content);
      const embeddingResults = await generateEmbeddings(chunkContents);
      console.log(`  üî¢ Generated ${embeddingResults.length} embeddings`);

      // Insert chunks with embeddings (batch insert with validation)
      const chunksToInsert = [];
      for (let i = 0; i < documentChunks.length; i++) {
        const chunk = documentChunks[i];
        const embedding = embeddingResults[i]?.embedding;

        if (!embedding) {
          console.warn(`  ‚ö†Ô∏è  Missing embedding for chunk ${i}`);
          continue;
        }

        chunksToInsert.push({
          documentId: existingDoc.id,
          chunkIndex: chunk.chunkIndex,
          content: chunk.content,
          sectionTitle: chunk.sectionTitle,
          embedding: embedding,
        });
      }

      if (chunksToInsert.length > 0) {
        // Use validated insert - will catch invalid embeddings (wrong dimensions, etc.)
        try {
          await insertChunks(chunksToInsert);
          totalChunks += chunksToInsert.length;
          totalEmbeddings += chunksToInsert.length;
        } catch (error) {
          if (error instanceof Error && error.name === "ZodError") {
            console.error(
              `  ‚ùå Validation failed for chunks in ${file}:`,
              error.message,
            );
            throw error;
          }
          throw error;
        }
      }

      console.log(`  ‚úÖ Completed\n`);
    }

    // Summary
    console.log("‚îÅ".repeat(50));
    console.log("üìä Seeding Summary:");
    console.log(`  Documents processed: ${totalDocuments}`);
    console.log(`  Documents skipped (duplicates): ${skippedDuplicates}`);
    console.log(`  Chunks created: ${totalChunks}`);
    console.log(`  Embeddings generated: ${totalEmbeddings}`);
    console.log("‚îÅ".repeat(50));
    console.log("‚ú® Seeding completed successfully!\n");
  } catch (error) {
    console.error("\n‚ùå Error during seeding:", error);
    process.exit(1);
  }
}

seed();
